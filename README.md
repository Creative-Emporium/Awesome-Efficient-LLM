# Awesome-Efficient-LLM


-[Awesome Efficient Large Language Models]
  - [Knowledge Distillation]
  - [Pruning]
  - [Quantization]
  - [Others]

[Github]() \| [Paper]()
[![Star](https://img.shields.io/github/stars/
.svg?style=social&label=Star)](https://github.com/)


## Knowledge Distllation
| Title & Authors | Introduction | Links |
|:----|  :----: | :---:|
|<br> [LaMini-LM](https://github.com/mbzuai-nlp/LaMini-LM) [![Star](https://img.shields.io/github/stars/mbzuai-nlp/LaMini-LM.svg?style=social&label=Star)](https://github.com/mbzuai-nlp/LaMini-LM) </br>Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, Alham Fikri Aji | <img width="1002" alt="image" src="https://github.com/mbzuai-nlp/LaMini-LM/blob/main/images/lamini-pipeline.drawio.png"> | [Github](https://github.com/mbzuai-nlp/LaMini-LM) \| [Paper](https://arxiv.org/abs/2304.14402) |
|<br> [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301) </br> Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister | <img width="2000" alt="image" src="https://github.com/horseee/Awesome-Efficient-LLM/assets/22924514/dbb6bb05-8d75-4248-af64-d5eaf9c09e5f">| [Paper](https://arxiv.org/abs/2305.02301) | 

## Pruning
| Title & Authors | Introduction | Links |
|:----|  :----: | :---:|
|<br>[LLM-Pruner: On the Structural Pruning of Large Language Models](https://arxiv.org/abs/2305.11627)[![Star](https://img.shields.io/github/stars/horseee/LLM-Pruner.svg?style=social&label=Star)](https://github.com/horseee/LLM-Pruner)</br> Xinyin Ma, Gongfan Fang, Xinchao Wang | <img width="1061" alt="image" src="https://github.com/horseee/Awesome-Efficient-LLM/assets/22924514/5aaef9e0-ab62-4f5f-b70d-cc3033d7d511">| [Github](https://github.com/horseee/LLM-Pruner) \| [Paper](https://arxiv.org/abs/2305.11627)|


## Quantization
| Title & Authors | Introduction | Links |
|:--|  :----: | :---:|
| [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa) [![Star](https://img.shields.io/github/stars/qwopqwop200/GPTQ-for-LLaMa.svg?style=social&label=Star)](https://github.com/qwopqwop200/GPTQ-for-LLaMa): 4 bits quantization of LLaMA using GPTQ. | <img width="102" alt="image" src="https://user-images.githubusercontent.com/64115820/235287009-2d07bba8-9b85-4973-9e06-2a3c28777f06.png"> |[Github](https://github.com/qwopqwop200/GPTQ-for-LLaMa)|
| <br>[Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling](https://arxiv.org/abs/2304.09145v1) </br> Xiuying Wei , Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu|  <img width="1102" alt="image" src="https://github.com/horseee/Awesome-Efficient-LLM/assets/22924514/bf69bf85-aabd-49ba-a6ff-164513ce48d5"> | [Paper](https://arxiv.org/abs/2304.09145v1)|
| <br>[RPTQ: Reorder-based Post-training Quantization for Large Language Models](https://arxiv.org/abs/2304.01089) [![Star](https://img.shields.io/github/stars/hahnyuan/RPTQ4LLM.svg?style=social&label=Star)](https://github.com/hahnyuan/RPTQ4LLM)</br> Zhihang Yuan and Lin Niu and Jiawei Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe Wu | ![](https://github.com/hahnyuan/RPTQ4LLM/blob/master/ims/cover.png) | <br>[Github](https://github.com/hahnyuan/RPTQ4LLM)</br> [Paper](https://arxiv.org/abs/2304.01089) |
