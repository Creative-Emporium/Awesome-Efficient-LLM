# Project for Efficient LLM

## Tools
* [![Star](https://img.shields.io/github/stars/vllm-project/vllm.svg?style=social&label=Star)](https://github.com/vllm-project/vllm) **vllm**: A high-throughput and memory-efficient inference and serving engine for LLMs. [[link]](https://github.com/vllm-project/vllm)[[paper]](https://arxiv.org/abs/2309.06180)
* [![Star](https://img.shields.io/github/stars/TimDettmers/bitsandbytes.svg?style=social&label=Star)](https://github.com/TimDettmers/bitsandbytes) **bitsandbytes**: 8-bit CUDA functions for PyTorch. [[link]](https://github.com/TimDettmers/bitsandbytes)
* [![Star](https://img.shields.io/github/stars/mit-han-lab/TinyChatEngine.svg?style=social&label=Star)](https://github.com/mit-han-lab/TinyChatEngine) **TinyChatEngine**: TinyChatEngine: On-Device LLM Inference Library. [[link]](https://github.com/mit-han-lab/TinyChatEngine)
* [![Star](https://img.shields.io/github/stars/microsoft/LMOps.svg?style=social&label=Star)](https://github.com/microsoft/LMOps) **LMOps**: General technology for enabling AI capabilities w/ LLMs and MLLMs. [[link]](https://github.com/microsoft/LMOps)
* [![Star](https://img.shields.io/github/stars/Lightning-AI/lit-gpt.svg?style=social&label=Star)](https://github.com/Lightning-AI/lit-gpt) **lit-gpt**: Hackable implementation of state-of-the-art open-source LLMs based on nanoGPT. Supports flash attention, 4-bit and 8-bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.. [[link]](https://github.com/Lightning-AI/lit-gpt)
* [![Star](https://img.shields.io/github/stars/ztxz16/fastllm.svg?style=social&label=Star)](https://github.com/ztxz16/fastllm) **fastllm**: 纯c++的全平台llm加速库，支持python调用，chatglm-6B级模型单卡可达10000+token / s，支持glm, llama, moss基座，手机端流畅运行. [[link]](https://github.com/ztxz16/fastllm)



## Open-source Lightweight LLM
* [![Star](https://img.shields.io/github/stars/jzhang38/TinyLlama.svg?style=social&label=Star)](https://github.com/jzhang38/TinyLlama) **TinyLlama**: The TinyLlama project is an open endeavor to pretrain a 1.1B Llama model on 3 trillion tokens.. [[link]](https://github.com/jzhang38/TinyLlama)
