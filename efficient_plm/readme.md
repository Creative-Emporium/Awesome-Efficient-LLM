# Efficient-PLM

A curated list for **Efficient Pre-trained Language Models**:
  - [Knowledge Distillation](#knowledge-distillation)
  - [Network Pruning](#network-pruning)
  - [Quantization](#quantization)
  - [Inference Acceleration](#inference-acceleration)
  - [Others](#others)

This part is still under construction to include papers published from 2018-2023.



## Knowledge Distillation
* 

## Network Pruning
* [![Publish](https://img.shields.io/badge/Conference-ACL'23-blue)]() [![Star](https://img.shields.io/github/stars/kongds/SMP.svg?style=social&label=Star)](https://github.com/allenai/kongds/SMP) [Pruning Pre-trained Language Models Without Fine-Tuning](https://aclanthology.org/2023.acl-long.35/). Ting Jiang, Deqing Wang, Fuzhen Zhuang, Ruobing Xie, Feng Xia. [[Paper]](https://aclanthology.org/2023.acl-long.35/)) [[Github]](https://github.com/kongds/SMP) 
* [![Publish](https://img.shields.io/badge/Conference-ACL'23%20Findings-blue)]() [Structured Pruning for Efficient Generative Pre-trained Language Models](https://aclanthology.org/2023.findings-acl.692/). 
Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong. [[Paper]](https://aclanthology.org/2023.findings-acl.692/) 

## Quantization

## Inference Acceleration

## Others
