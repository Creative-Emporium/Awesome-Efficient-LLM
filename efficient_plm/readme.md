# Efficient-PLM

A curated list for **Efficient Pre-trained Language Models**:
  - [Knowledge Distillation](#knowledge-distillation)
  - [Network Pruning](#network-pruning)
  - [Quantization](#quantization)
  - [Inference Acceleration](#inference-acceleration)
  - [Others](#others)

This part is still under construction to include papers published from 2018-2023.



## Knowledge Distillation
* 

## Network Pruning
* [![Publish](https://img.shields.io/badge/Conference-ACL'23-blue)]() [![Star](https://img.shields.io/github/stars/kongds/SMP.svg?style=social&label=Star)](https://github.com/allenai/kongds/SMP) [Pruning Pre-trained Language Models Without Fine-Tuning](https://aclanthology.org/2023.acl-long.35/). Ting Jiang, Deqing Wang, Fuzhen Zhuang, Ruobing Xie, Feng Xia. [[Paper]](https://aclanthology.org/2023.acl-long.35/)) [[Github]](https://github.com/kongds/SMP) 
* [![Publish](https://img.shields.io/badge/Conference-ACL'23-blue)]() [![Star](https://img.shields.io/github/stars/airaria/GRAIN.svg?style=social&label=Star)](https://github.com/airaria/GRAIN) [Gradient-based Intra-attention Pruning on Pre-trained Language Models](https://arxiv.org/abs/2212.07634). Ziqing Yang, Yiming Cui, Xin Yao, Shijin Wang. [[Paper]](https://arxiv.org/abs/2212.07634)[[Github]](https://github.com/airaria/GRAIN)
* [![Publish](https://img.shields.io/badge/Conference-ACL'23-blue)]() [![Star](https://img.shields.io/github/stars/naver/nllb-pruning.svg?style=social&label=Star)](https://github.com/naver/nllb-pruning) [Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model](https://arxiv.org/abs/2212.09811). Yeskendir Koishekenov, Alexandre Berard, Vassilina Nikoulina. [[Paper]](https://arxiv.org/abs/2212.09811)[[Github]](https://github.com/naver/nllb-pruning)
* [![Publish](https://img.shields.io/badge/Conference-ACL'23%20Findings-blue)]() [Structured Pruning for Efficient Generative Pre-trained Language Models](https://aclanthology.org/2023.findings-acl.692/). 
Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong. [[Paper]](https://aclanthology.org/2023.findings-acl.692/) 

## Quantization
* [![Publish](https://img.shields.io/badge/Conference-ACL'23-blue)]() [Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models](https://aclanthology.org/2023.acl-short.114/). James Oâ€™Neill, Sourav Dutta. [[Paper]](https://aclanthology.org/2023.acl-short.114/)

## Inference Acceleration
* [![Publish](https://img.shields.io/badge/Conference-ACL'23-blue)]() [![Star](https://img.shields.io/github/stars/dropreg/DEER.svg?style=social&label=Star)](https://github.com/dropreg/DEER) [Dynamic and Efficient Inference for Text Generation via BERT Family](https://aclanthology.org/2023.acl-long.162/). Xiaobo Liang, Juntao Li, Lijun Wu, Ziqiang Cao, Min Zhang. [[Paper]](https://aclanthology.org/2023.acl-long.162/)[[Github]](https://github.com/dropreg/DEER)

## Others
* [![Publish](https://img.shields.io/badge/Conference-ACL'23-blue)]() [PuMer: Pruning and Merging Tokens for Efficient Vision Language Models](https://arxiv.org/abs/2305.17530). Qingqing Cao, Bhargavi Paranjape, Hannaneh Hajishirzi. [[Paper]](https://arxiv.org/abs/2305.17530)